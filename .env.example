# .env - copy from .env.example and fill in values

# Pi-hole Configuration
PIHOLE_HOST=192.168.7.99
PIHOLE_PORT=8080                     # default FTL port (80 or 8080 on your system)
PIHOLE_PASSWORD=your_pw_here

# LLM Provider Configuration (choose one)

# Option 1: Ollama (default - local LLM hosting)
OLLAMA_URL=http://localhost:11434    # Ollama local REST API
OLLAMA_MODEL=llama3.2                # change to a model you have installed

# Option 2: OpenAI (alternative - requires API key)
# LLM_PROVIDER=openai
# LLM_API_KEY=your_openai_api_key_here
# LLM_MODEL=gpt-3.5-turbo
# LLM_API_BASE_URL=https://api.openai.com/v1

# General Configuration
LOG_COUNT=100                        # how many logs to fetch (max)
MAX_PROMPT_CHARS=18000               # truncate logs to keep prompt within limits
LLM_TIMEOUT=120                      # request timeout in seconds
LLM_TEMPERATURE=0.2                  # response creativity (0.0-1.0)
LLM_MAX_TOKENS=512                   # response length limit
